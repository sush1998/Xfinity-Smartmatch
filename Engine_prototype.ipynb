{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/narengarapati/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/narengarapati/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Connect       1.00      1.00      1.00       135\n",
      "Connect More       1.00      1.00      1.00       323\n",
      "        Fast       1.00      1.00      1.00        98\n",
      " Gigabit Pro       1.00      1.00      1.00       565\n",
      "  Gigabit x2       1.00      1.00      1.00        70\n",
      "   Superfast       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00      1284\n",
      "   macro avg       1.00      1.00      1.00      1284\n",
      "weighted avg       1.00      1.00      1.00      1284\n",
      "\n",
      "\n",
      "Test Recommendation:\n",
      "Prompt: We're a family of 4 with 8 devices. We stream HD movies, play online games, and I work from home. We need reliable internet but don't want to break the bank.\n",
      "Recommended Plan: Gigabit Pro\n",
      "Explanation: Based on your household size and number of connected devices and internet usage patterns, the Gigabit Pro plan would be best for your needs.\n",
      "Speed: 6000 Mbps\n",
      "Price: $299.95\n"
     ]
    }
   ],
   "source": [
    "# recommender enginer 4:40 pm \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model for NLP preprocessing\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    # If model not available, download it\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class XfinityPlanRecommender:\n",
    "    def __init__(self, data_path, plans_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the recommender system.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the training data JSON\n",
    "            plans_path: Optional path to Xfinity plans data\n",
    "        \"\"\"\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "        # Load training data\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "        # Convert to DataFrame for easier handling\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "        # Load plans data if provided\n",
    "        if plans_path:\n",
    "            with open(plans_path, 'r') as f:\n",
    "                self.plans_data = json.load(f)\n",
    "                \n",
    "        # Prepare the model pipeline\n",
    "        self.model = None\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text for model input:\n",
    "        - Lower case\n",
    "        - Remove special characters\n",
    "        - Lemmatize words\n",
    "        - Remove stopwords\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Process with spaCy for better entity recognition\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extract key information and lemmatize\n",
    "        processed_text = []\n",
    "        for token in doc:\n",
    "            if token.text not in self.stopwords and not token.is_punct:\n",
    "                lemma = self.lemmatizer.lemmatize(token.text)\n",
    "                processed_text.append(lemma)\n",
    "                \n",
    "        return ' '.join(processed_text)\n",
    "    \n",
    "    def extract_requirements_features(self, row):\n",
    "        \"\"\"Extract structured features from the requirements field\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Extract household size\n",
    "        features.append(f\"household_{row['requirements']['household_size']}\")\n",
    "        \n",
    "        # Extract device count\n",
    "        features.append(f\"devices_{row['requirements']['device_count']}\")\n",
    "        \n",
    "        # Extract use cases\n",
    "        for use_case in row['requirements']['use_cases']:\n",
    "            features.append(f\"usecase_{use_case.replace(' ', '_')}\")\n",
    "            \n",
    "        # Extract budget level\n",
    "        features.append(f\"budget_{row['requirements']['budget_level']}\")\n",
    "        \n",
    "        # Extract data usage\n",
    "        features.append(f\"high_data_{row['requirements']['high_data_usage']}\")\n",
    "        \n",
    "        return ' '.join(features)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Preprocess prompts\n",
    "        self.df['processed_prompt'] = self.df['prompt'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Extract features from requirements if needed\n",
    "        # This can be used for a multi-input model\n",
    "        self.df['requirement_features'] = self.df.apply(self.extract_requirements_features, axis=1)\n",
    "        \n",
    "        # Combine features (text + extracted features)\n",
    "        self.df['combined_features'] = self.df['processed_prompt'] + ' ' + self.df['requirement_features']\n",
    "        \n",
    "        # Split data\n",
    "        X = self.df['combined_features']\n",
    "        y = self.df['plan']\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the recommendation model\"\"\"\n",
    "        # Create a pipeline with TF-IDF and Random Forest\n",
    "        self.model = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                analyzer='word',\n",
    "                max_features=1000,\n",
    "                ngram_range=(1, 2)  # Include bigrams\n",
    "            )),\n",
    "            ('classifier', RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        report = classification_report(self.y_test, y_pred)\n",
    "        \n",
    "        print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def predict(self, user_prompt):\n",
    "        \"\"\"\n",
    "        Predict the best Xfinity plan based on a user prompt\n",
    "        \n",
    "        Args:\n",
    "            user_prompt: String describing user's internet needs\n",
    "            \n",
    "        Returns:\n",
    "            Recommended plan name\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
    "        \n",
    "        # Preprocess the input prompt\n",
    "        processed_prompt = self.preprocess_text(user_prompt)\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_plan = self.model.predict([processed_prompt])[0]\n",
    "        \n",
    "        return predicted_plan\n",
    "    \n",
    "    def explain_recommendation(self, user_prompt):\n",
    "        \"\"\"\n",
    "        Provide a plan recommendation with explanation\n",
    "        \n",
    "        Args:\n",
    "            user_prompt: String describing user's internet needs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with recommendation and explanation\n",
    "        \"\"\"\n",
    "        plan_name = self.predict(user_prompt)\n",
    "        \n",
    "        # Find the plan details from the plans data\n",
    "        plan_details = None\n",
    "        if hasattr(self, 'plans_data'):\n",
    "            for plan in self.plans_data['internet']:\n",
    "                if plan['name'] == plan_name:\n",
    "                    plan_details = plan\n",
    "                    break\n",
    "        \n",
    "        # Extract key information from the prompt\n",
    "        doc = nlp(user_prompt)\n",
    "        \n",
    "        # Try to identify key factors in the decision\n",
    "        factors = []\n",
    "        \n",
    "        # Check for keywords related to household size\n",
    "        if any(word in user_prompt.lower() for word in ['family', 'household', 'people', 'kids']):\n",
    "            factors.append(\"household size\")\n",
    "        \n",
    "        # Check for keywords related to devices\n",
    "        if any(word in user_prompt.lower() for word in ['device', 'devices', 'connected']):\n",
    "            factors.append(\"number of connected devices\")\n",
    "        \n",
    "        # Check for keywords related to usage\n",
    "        if any(word in user_prompt.lower() for word in ['stream', 'gaming', 'video', 'work', 'business']):\n",
    "            factors.append(\"internet usage patterns\")\n",
    "        \n",
    "        # Check for keywords related to budget\n",
    "        if any(word in user_prompt.lower() for word in ['budget', 'affordable', 'cheap', 'expensive', 'price']):\n",
    "            factors.append(\"budget considerations\")\n",
    "        \n",
    "        # Check for keywords related to data usage\n",
    "        if any(word in user_prompt.lower() for word in ['data', 'cap', 'unlimited']):\n",
    "            factors.append(\"data usage requirements\")\n",
    "        \n",
    "        # Construct explanation\n",
    "        explanation = f\"Based on your {' and '.join(factors)}, the {plan_name} plan would be best for your needs.\"\n",
    "        \n",
    "        result = {\n",
    "            \"recommended_plan\": plan_name,\n",
    "            \"explanation\": explanation,\n",
    "            \"plan_details\": plan_details\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create recommender instance\n",
    "    training_data = '/Users/narengarapati/Desktop/xfinity_training_data_2.json'\n",
    "    target = '/Users/narengarapati/Desktop/Xfinity_data.json'\n",
    "    recommender = XfinityPlanRecommender(training_data, target)\n",
    "    \n",
    "    # Prepare data and train model\n",
    "    recommender.prepare_data()\n",
    "    recommender.train()\n",
    "    \n",
    "    # Test with a new prompt\n",
    "    test_prompt = \"We're a family of 4 with 8 devices. We stream HD movies, play online games, and I work from home. We need reliable internet but don't want to break the bank.\"\n",
    "    recommendation = recommender.explain_recommendation(test_prompt)\n",
    "    \n",
    "    print(\"\\nTest Recommendation:\")\n",
    "    print(f\"Prompt: {test_prompt}\")\n",
    "    print(f\"Recommended Plan: {recommendation['recommended_plan']}\")\n",
    "    print(f\"Explanation: {recommendation['explanation']}\")\n",
    "    if recommendation['plan_details']:\n",
    "        print(f\"Speed: {recommendation['plan_details']['download_speed']}\")\n",
    "        print(f\"Price: {recommendation['plan_details']['price']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef0ef79014c4adab1b6639254082d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3704263b0b4d9399c8f37c8ef696b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db0c96177714b2d8e8042f2f7f4757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e446ef31344e11b88a023a6448ec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Prepared 5134 training samples and 1284 validation samples\n",
      "Label mapping: {'Gigabit x2': 0, 'Connect': 1, 'Connect More': 2, 'Gigabit Pro': 3, 'Fast': 4, 'Superfast': 5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd24260b5cb244e1894c1370f4040481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/3\n",
      "Average training loss: 0.2339244163689433\n",
      "Validation Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Connect       1.00      1.00      1.00       135\n",
      "Connect More       1.00      1.00      1.00       323\n",
      "        Fast       1.00      1.00      1.00        98\n",
      " Gigabit Pro       1.00      1.00      1.00       565\n",
      "  Gigabit x2       1.00      1.00      1.00        70\n",
      "   Superfast       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00      1284\n",
      "   macro avg       1.00      1.00      1.00      1284\n",
      "weighted avg       1.00      1.00      1.00      1284\n",
      "\n",
      "Starting epoch 2/3\n",
      "Average training loss: 0.006219782250007829\n",
      "Validation Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Connect       1.00      1.00      1.00       135\n",
      "Connect More       1.00      1.00      1.00       323\n",
      "        Fast       1.00      1.00      1.00        98\n",
      " Gigabit Pro       1.00      1.00      1.00       565\n",
      "  Gigabit x2       1.00      1.00      1.00        70\n",
      "   Superfast       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00      1284\n",
      "   macro avg       1.00      1.00      1.00      1284\n",
      "weighted avg       1.00      1.00      1.00      1284\n",
      "\n",
      "Starting epoch 3/3\n",
      "Average training loss: 0.003753478011055458\n",
      "Validation Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Connect       1.00      1.00      1.00       135\n",
      "Connect More       1.00      1.00      1.00       323\n",
      "        Fast       1.00      1.00      1.00        98\n",
      " Gigabit Pro       1.00      1.00      1.00       565\n",
      "  Gigabit x2       1.00      1.00      1.00        70\n",
      "   Superfast       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00      1284\n",
      "   macro avg       1.00      1.00      1.00      1284\n",
      "weighted avg       1.00      1.00      1.00      1284\n",
      "\n",
      "Training complete!\n",
      "Extracted features: {'household_size': 3, 'device_count': 8, 'use_cases': ['streaming', '4K streaming', 'gaming', 'work from home'], 'budget_level': 'mid_range', 'high_data_usage': False}\n",
      "\n",
      "Recommended Plan: Fast\n"
     ]
    }
   ],
   "source": [
    "# improved version: \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "class XfinityPlanRecommenderAdvanced:\n",
    "    def __init__(self, data_path, plans_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the advanced recommender system using BERT.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the training data JSON\n",
    "            plans_path: Optional path to Xfinity plans data\n",
    "        \"\"\"\n",
    "        # Load training data\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "        # Convert to DataFrame for easier handling\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "        # Load plans data if provided\n",
    "        if plans_path:\n",
    "            with open(plans_path, 'r') as f:\n",
    "                self.plans_data = json.load(f)\n",
    "        \n",
    "        # Set up BERT tokenizer and model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Check if CUDA is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize model as None until we train\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data for BERT model\"\"\"\n",
    "        # Get unique plan labels\n",
    "        unique_plans = self.df['plan'].unique()\n",
    "        self.label_dict = {plan: i for i, plan in enumerate(unique_plans)}\n",
    "        self.reverse_label_dict = {i: plan for plan, i in self.label_dict.items()}\n",
    "        \n",
    "        # Convert plan names to numeric labels\n",
    "        self.df['label'] = self.df['plan'].map(self.label_dict)\n",
    "        \n",
    "        # Combine prompt with structured data for more context\n",
    "        self.df['combined_text'] = self.df.apply(\n",
    "            lambda x: f\"{x['prompt']} Household size: {x['requirements']['household_size']}. \" +\n",
    "                     f\"Devices: {x['requirements']['device_count']}. \" +\n",
    "                     f\"Uses: {', '.join(x['requirements']['use_cases'])}. \" +\n",
    "                     f\"Budget: {x['requirements']['budget_level']}. \" +\n",
    "                     f\"High data usage: {x['requirements']['high_data_usage']}.\",\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Create train/validation split\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            self.df['combined_text'].values, \n",
    "            self.df['label'].values,\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Tokenize and encode sequences\n",
    "        self.train_encodings = self.tokenizer(\n",
    "            train_texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        self.val_encodings = self.tokenizer(\n",
    "            val_texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convert to PyTorch datasets\n",
    "        self.train_dataset = TensorDataset(\n",
    "            self.train_encodings['input_ids'],\n",
    "            self.train_encodings['attention_mask'],\n",
    "            torch.tensor(train_labels)\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = TensorDataset(\n",
    "            self.val_encodings['input_ids'],\n",
    "            self.val_encodings['attention_mask'],\n",
    "            torch.tensor(val_labels)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        self.train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=RandomSampler(self.train_dataset),\n",
    "            batch_size=8\n",
    "        )\n",
    "        \n",
    "        self.val_dataloader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            sampler=SequentialSampler(self.val_dataset),\n",
    "            batch_size=8\n",
    "        )\n",
    "        \n",
    "        print(f\"Prepared {len(self.train_dataset)} training samples and {len(self.val_dataset)} validation samples\")\n",
    "        print(f\"Label mapping: {self.label_dict}\")\n",
    "    \n",
    "    def train(self, epochs=4):\n",
    "        \"\"\"Train the BERT model\"\"\"\n",
    "        # Initialize the BERT model for sequence classification\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels=len(self.label_dict),\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
    "        \n",
    "        # Set up learning rate scheduler\n",
    "        total_steps = len(self.train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Set model to training mode\n",
    "            self.model.train()\n",
    "            \n",
    "            total_loss = 0\n",
    "            \n",
    "            # Process batches\n",
    "            for batch in self.train_dataloader:\n",
    "                # Add batch to device\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                \n",
    "                # Clear gradients\n",
    "                self.model.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            avg_train_loss = total_loss / len(self.train_dataloader)\n",
    "            print(f\"Average training loss: {avg_train_loss}\")\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            \n",
    "            val_preds = []\n",
    "            val_true = []\n",
    "            \n",
    "            for batch in self.val_dataloader:\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    )\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                val_preds.extend(predictions)\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            accuracy = accuracy_score(val_true, val_preds)\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Convert numeric predictions back to plan names for the report\n",
    "            val_preds_names = [self.reverse_label_dict[p] for p in val_preds]\n",
    "            val_true_names = [self.reverse_label_dict[t] for t in val_true]\n",
    "            \n",
    "            report = classification_report(val_true_names, val_preds_names)\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(report)\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, user_prompt, include_requirements=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Predict the best Xfinity plan based on a user prompt\n",
    "        \n",
    "        Args:\n",
    "            user_prompt: String describing user's internet needs\n",
    "            include_requirements: Whether to include structured requirements in prediction\n",
    "            **kwargs: Optional requirements parameters (household_size, device_count, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Recommended plan name\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
    "        \n",
    "        # Process the input text\n",
    "        if include_requirements and kwargs:\n",
    "            # Extract requirements from kwargs\n",
    "            household_size = kwargs.get('household_size', 1)\n",
    "            device_count = kwargs.get('device_count', 1)\n",
    "            use_cases = kwargs.get('use_cases', ['basic browsing'])\n",
    "            budget_level = kwargs.get('budget_level', 'economy')\n",
    "            high_data_usage = kwargs.get('high_data_usage', False)\n",
    "            \n",
    "            # Combine prompt with requirements\n",
    "            text = f\"{user_prompt} Household size: {household_size}. \" + \\\n",
    "                   f\"Devices: {device_count}. \" + \\\n",
    "                   f\"Uses: {', '.join(use_cases)}. \" + \\\n",
    "                   f\"Budget: {budget_level}. \" + \\\n",
    "                   f\"High data usage: {high_data_usage}.\"\n",
    "        else:\n",
    "            text = user_prompt\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Convert numeric prediction to plan name\n",
    "        predicted_plan = self.reverse_label_dict[prediction]\n",
    "        \n",
    "        return predicted_plan\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the model for later use\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"No model to save. Train the model first.\")\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save_pretrained(path)\n",
    "        \n",
    "        # Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Save label mappings\n",
    "        with open(f\"{path}/label_dict.json\", 'w') as f:\n",
    "            json.dump(self.label_dict, f)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a previously saved model\"\"\"\n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(path)\n",
    "        \n",
    "        # Load label mappings\n",
    "        with open(f\"{path}/label_dict.json\", 'r') as f:\n",
    "            self.label_dict = json.load(f)\n",
    "            \n",
    "        # Convert string keys to integers for reverse mapping\n",
    "        self.reverse_label_dict = {int(i): plan for plan, i in self.label_dict.items()}\n",
    "        \n",
    "        # Load the model\n",
    "        self.model = BertForSequenceClassification.from_pretrained(path)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "# Example usage with feature extraction from prompt\n",
    "def extract_features_from_prompt(prompt):\n",
    "    \"\"\"\n",
    "    Attempt to extract structured features from a natural language prompt\n",
    "    This would be used when we don't have structured requirements data\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    features = {\n",
    "        'household_size': 1,\n",
    "        'device_count': 1,\n",
    "        'use_cases': ['basic browsing'],\n",
    "        'budget_level': 'economy',\n",
    "        'high_data_usage': False\n",
    "    }\n",
    "    \n",
    "    # Extract household size\n",
    "    household_match = re.search(r'(\\d+)\\s+(people|person|family|families|household|households|member|members)', prompt, re.IGNORECASE)\n",
    "    if household_match:\n",
    "        features['household_size'] = int(household_match.group(1))\n",
    "    elif any(word in prompt.lower() for word in ['family', 'families', 'household', 'households']):\n",
    "        features['household_size'] = 3  # Default for family/household mentions\n",
    "    \n",
    "    # Extract device count\n",
    "    device_match = re.search(r'(\\d+)\\s+(device|devices|connected)', prompt, re.IGNORECASE)\n",
    "    if device_match:\n",
    "        features['device_count'] = int(device_match.group(1))\n",
    "    \n",
    "    # Extract use cases\n",
    "    use_cases = []\n",
    "    if any(word in prompt.lower() for word in ['stream', 'streaming', 'netflix', 'hulu', 'disney']):\n",
    "        use_cases.append('streaming')\n",
    "    if any(word in prompt.lower() for word in ['4k', 'hd', 'high definition', 'high-definition']):\n",
    "        use_cases.append('4K streaming')\n",
    "    if any(word in prompt.lower() for word in ['game', 'gaming', 'play']):\n",
    "        use_cases.append('gaming')\n",
    "    if any(word in prompt.lower() for word in ['competitive', 'tournament', 'esports']):\n",
    "        use_cases.append('competitive gaming')\n",
    "    if any(word in prompt.lower() for word in ['work', 'business', 'webinar', 'zoom', 'meeting']):\n",
    "        use_cases.append('work from home')\n",
    "    \n",
    "    if use_cases:\n",
    "        features['use_cases'] = use_cases\n",
    "    \n",
    "    # Extract budget level\n",
    "    if any(word in prompt.lower() for word in ['cheap', 'budget', 'affordable', 'save', 'tight']):\n",
    "        features['budget_level'] = 'economy'\n",
    "    elif any(word in prompt.lower() for word in ['premium', 'best', 'top', 'high end', 'high-end']):\n",
    "        features['budget_level'] = 'premium'\n",
    "    else:\n",
    "        features['budget_level'] = 'mid_range'\n",
    "    \n",
    "    # Extract data usage\n",
    "    if any(phrase in prompt.lower() for phrase in ['lot of data', 'high data', 'unlimited data', 'data cap']):\n",
    "        features['high_data_usage'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Full example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create advanced recommender instance\n",
    "    training_data = '/Users/narengarapati/Desktop/xfinity_training_data_2.json'\n",
    "    target = '/Users/narengarapati/Desktop/Xfinity_data.json'\n",
    "    recommender = XfinityPlanRecommenderAdvanced(training_data, target)\n",
    "    \n",
    "    # Prepare data and train model\n",
    "    recommender.prepare_data()\n",
    "    recommender.train(epochs=3)\n",
    "    \n",
    "    # Save the model\n",
    "    recommender.save_model(\"xfinity_bert_model\")\n",
    "    \n",
    "    # Load the model (would be used in production)\n",
    "    # recommender.load_model(\"xfinity_bert_model\")\n",
    "    \n",
    "    # Test with a new prompt\n",
    "    test_prompt = \"We're a family of 4 with 8 devices. We stream HD movies, play online games, and I work from home. We need reliable internet but don't want to break the bank.\"\n",
    "    \n",
    "    # Extract features from the prompt\n",
    "    features = extract_features_from_prompt(test_prompt)\n",
    "    print(f\"Extracted features: {features}\")\n",
    "    \n",
    "    # Make prediction using extracted features\n",
    "    predicted_plan = recommender.predict(\n",
    "        test_prompt,\n",
    "        include_requirements=True,\n",
    "        **features\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecommended Plan: {predicted_plan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
